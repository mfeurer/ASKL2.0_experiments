{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.transforms\n",
    "import getpass\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import openml\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "from plot_utils.style import style_dc\n",
    "from plot_utils.common_plots import rank, average_plot\n",
    "from plot_utils.common_tables import collect_data_for_final_table, do_wilcoxon_test\n",
    "from plot_utils.common_loading import load_from_openml\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/eggenspk/Work/Project/2020_PoSH_Autosklearn/2020_IEEE_Autosklearn_experiments/experiment_scripts/\")\n",
    "sys.path.append(\"/home/feurerm/sync_dir/projects/2020_posh/2020_IEEE_Autosklearn_experiments/experiment_scripts\")\n",
    "from utils import openml_automl_benchmark, get_normalization_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "username = getpass.getuser()\n",
    "dir_ = {\n",
    "    'eggenspk': \"/home/eggenspk/Work/Project/2020_PoSH_Autosklearn/DATA/\",\n",
    "    'feurerm': \"/home/feurerm/projects/2020_posh/\",\n",
    "}[username]\n",
    "valid_pretty = {\n",
    "    10: {\n",
    "    (\"auto\", True, \"10MIN/RQ1_AutoAuto_simulate/dynamic/60/autoauto\"): \"Auto-sklearn (2.0)\",\n",
    "    (None, True, \"10MIN/ASKL_automldata_baseline_full/RF/RF_None_holdout_full_es_nif\"): \"Auto-sklearn (1.0)\",\n",
    "    (None, True, \"10MIN/ASKL_automldata_baseline_full_no_metalearning/RF/RF_None_holdout_full_es_nif\"): \"Auto-sklearn (1.0, no MtL)\",\n",
    "    (None, True, \"10MIN/ASKL_automldata_baseline_full_random/None/None_None_holdout_full_es_nif\"): \"Auto-sklearn (1.0, random search)\",\n",
    "    (None, True, \"10MIN/ASKL_automldata_baseline_iter/RF/RF_None_holdout_iterative_es_nif\"): \"Auto-sklearn (1.0, iterative search space)\",\n",
    "    (None, True, \"10MIN/ASKL_automldata_baseline_iter_no_metalearning/RF/RF_None_holdout_iterative_es_nif\"): \"Auto-sklearn (1.0, iterative search space, no MtL)\",\n",
    "    (None, True, \"10MIN/ASKL_automldata_baseline_iter_random/None/None_None_holdout_iterative_es_nif\"): \"Auto-sklearn (1.0, random search, iterative search space)\",\n",
    "    (\"auto\", False, \"10MIN/RQ1_AutoAuto_simulate/dynamic/60/autoauto\"): \"Selector (ens in sel)\",\n",
    "    (\"auto\", False, \"10MIN/AutoAuto_simulate/dynamic/60/autoauto\"): \"Selector (no ens in sel)\",\n",
    "    (\"auto\", False, \"10MIN/AutoAuto_simulate_RQ1_target_dir/dynamic/60/autoauto\"): \"Selector (no ens in sel) - other dir\",\n",
    "    (None, False, \"10MIN/ASKL_automldata_baseline_full/RF/RF_None_holdout_full_es_nif\"): \"ASKL(full)\",\n",
    "    (None, False, \"10MIN/ASKL_automldata_baseline_full_random/None/None_None_holdout_full_es_nif\"): \"ASKL(full,random)\",\n",
    "    (None, False, \"10MIN/ASKL_automldata_baseline_iter/RF/RF_None_holdout_iterative_es_nif\"): \"ASKL(iter)\",\n",
    "    (None, False, \"10MIN/ASKL_automldata_baseline_iter_random/None/None_None_holdout_iterative_es_nif\"): \"ASKL(iter,random)\",\n",
    "    },\n",
    "    60: {\n",
    "    (\"auto\", True, \"60MIN/RQ1_AutoAuto_simulate/dynamic/360/autoauto\"): \"Auto-sklearn (2.0)\",\n",
    "    (None, True, \"60MIN/ASKL_automldata_baseline_full/RF/RF_None_holdout_full_es_nif\"): \"Auto-sklearn (1.0)\",\n",
    "    (None, True, \"60MIN/ASKL_automldata_baseline_full_no_metalearning/RF/RF_None_holdout_full_es_nif\"): \"Auto-sklearn (1.0, no MtL)\",\n",
    "    (None, True, \"60MIN/ASKL_automldata_baseline_full_random/None/None_None_holdout_full_es_nif\"): \"Auto-sklearn (1.0, random search)\",\n",
    "    (None, True, \"60MIN/ASKL_automldata_baseline_iter/RF/RF_None_holdout_iterative_es_nif\"): \"Auto-sklearn (1.0, iterative search space)\",\n",
    "    (None, True, \"60MIN/ASKL_automldata_baseline_iter_no_metalearning/RF/RF_None_holdout_iterative_es_nif\"): \"Auto-sklearn (1.0, iterative search space, no MtL)\",\n",
    "    (None, True, \"60MIN/ASKL_automldata_baseline_iter_random/None/None_None_holdout_iterative_es_nif\"): \"Auto-sklearn (1.0, random search, iterative search space)\",\n",
    "    (\"auto\", False, \"60MIN/RQ1_AutoAuto_simulate/dynamic/360/autoauto\"): \"Selector (ens in sel)\",\n",
    "    (\"auto\", False, \"60MIN/AutoAuto_simulate/dynamic/360/autoauto\"): \"Selector (no ens in sel)\",\n",
    "    (\"auto\", False, \"60MIN/AutoAuto_simulate_RQ1_target_dir/dynamic/360/autoauto\"): \"Selector (no ens in sel) - other dir\",\n",
    "    (None, False, \"60MIN/ASKL_automldata_baseline_full/RF/RF_None_holdout_full_es_nif\"): \"ASKL(full)\",\n",
    "    (None, False, \"60MIN/ASKL_automldata_baseline_full_random/None/None_None_holdout_full_es_nif\"): \"ASKL(full,random)\",\n",
    "    (None, False, \"60MIN/ASKL_automldata_baseline_iter/RF/RF_None_holdout_iterative_es_nif\"): \"ASKL(iter)\",\n",
    "    (None, False, \"60MIN/ASKL_automldata_baseline_iter_random/None/None_None_holdout_iterative_es_nif\"): \"ASKL(iter,random)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "task_ids = openml_automl_benchmark\n",
    "#for tid in [189873, 189874, 75193]:\n",
    "#    try:\n",
    "#        task_ids.remove(tid)\n",
    "#    except:\n",
    "#        pass\n",
    "print(task_ids, len(task_ids))\n",
    "\n",
    "res_dc = {}\n",
    "miss = 0\n",
    "fallback = 0\n",
    "for horizon in list(valid_pretty.keys()):\n",
    "    res_dc[horizon] = {}\n",
    "    for tid in task_ids:\n",
    "        res_dc[horizon][tid] = {}\n",
    "        for mode in list(valid_pretty[horizon].keys()):\n",
    "            auto, ensemble, model_name = mode\n",
    "            res_dc[horizon][tid][mode] = []\n",
    "            for seed in range(10):\n",
    "                if auto == \"auto\":\n",
    "                    if ensemble:\n",
    "                        fl_tmpl = dir_ + \"/\" + model_name + \"_%d_%d/ensemble_results_0.000000thresh_50size_1.000000best\" % (tid, seed)\n",
    "                    else:\n",
    "                        fl_tmpl = dir_ + \"/\" + model_name + \"_%d_%d/result.json\" % (tid, seed)\n",
    "\n",
    "                else:\n",
    "                    if ensemble:\n",
    "                        fl_tmpl = dir_ + \"/\" + model_name + \"_%d_%d_*_0/ensemble_results_0.000000thresh_50size_1.000000best\" % (tid, seed)\n",
    "                    else:\n",
    "                        fl_tmpl = dir_ + \"/\" + model_name + \"_%d_%d_*_0/result.json\" % (tid, seed)\n",
    "\n",
    "                fl = glob.glob(fl_tmpl)               \n",
    "                if len(fl) == 0:\n",
    "                    if auto == \"auto\":\n",
    "                        fl_tmpl = dir_ + \"/\" + model_name + \"_%d_%d/result.json\" % (tid, seed)\n",
    "                    else:\n",
    "                        fl_tmpl = dir_ + \"/\" + model_name + \"_%d_%d_*_0/result.json\" % (tid, seed)\n",
    "                    fl = glob.glob(fl_tmpl)\n",
    "                    if len(fl) == 0:                    \n",
    "                        miss += 1\n",
    "                        res_dc[horizon][tid][mode].append({0: 1})\n",
    "                        print('Missing', fl_tmpl)\n",
    "                        continue\n",
    "                    else:\n",
    "                        fallback += 1\n",
    "                        print('Ensemble missing, falling back to regular', fl[0])\n",
    "                        pass\n",
    "                fl = fl[0]\n",
    "                with open(fl, \"r\") as fh:\n",
    "                    try:\n",
    "                        line = json.load(fh)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        fh.seek(0)\n",
    "                        # Some files have two closing brackets...\n",
    "                        line = json.loads(fh.read()[:-1])\n",
    "                    except:\n",
    "                        print(fl)\n",
    "                        raise\n",
    "                    if \"50\" in line:\n",
    "                        loss = line[\"50\"][\"trajectory\"]\n",
    "                    else:\n",
    "                        loss = line[\"0\"][\"trajectory\"]\n",
    "                    loss = [(l[0], l[1]) for l in loss]\n",
    "                    loss = dict(loss)\n",
    "                    res_dc[horizon][tid][mode].append(loss)\n",
    "print(\"Missing %d entries\" % miss)\n",
    "print(\"Fallback %d entries\" % fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some stuff from disc and openml - takes some time\n",
    "tasks, task_ids_sorted_by_num_features = load_from_openml(task_ids)\n",
    "min_diff_dc = get_normalization_constants(dir_, load=True)\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\n",
    "    \"Auto-sklearn (2.0)\",\n",
    "    \"Auto-sklearn (1.0)\",\n",
    "    \"Auto-sklearn (1.0) \\nno KND\",\n",
    "    \"Auto-sklearn (1.0) \\nrandom search\", \n",
    "    \"Auto-sklearn (1.0) \\niterative search space\",\n",
    "    \"Auto-sklearn (1.0) \\nno KND & iterative search space\",\n",
    "    \"Auto-sklearn (1.0) \\nrandom search & iterative search space\",\n",
    ")\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "patches = [\n",
    "    mpatches.Patch(color=color, label=label)\n",
    "    for label, color in zip(labels, style_dc['colors'])]\n",
    "fig.legend(patches, labels, loc='upper left', frameon=True, ncol=2,\n",
    "           bbox_to_anchor=(0, 1.6, 0, 0), \n",
    "           borderaxespad=1., fontsize=style_dc[\"fontsize\"]*2)\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0 + 1.6, box.y0, box.width * 0.1, box.height * 0.1])\n",
    "#plt.show()\n",
    "#plt.tight_layout()\n",
    "plt.savefig('/tmp/RQ1_legend.pdf', bbox_inches=matplotlib.transforms.Bbox.from_bounds(0, 0, 22, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HORIZON = 60\n",
    "for tid in task_ids_sorted_by_num_features:\n",
    "    plt.figure(figsize=[16,12])\n",
    "    colors = itertools.cycle(style_dc['colors'])\n",
    "    for mode in list(valid_pretty[HORIZON].keys()):\n",
    "        c = next(colors)\n",
    "        tmp = pd.DataFrame(res_dc[HORIZON][tid][mode]).sort_index(axis=1).ffill(axis=1)\n",
    "        med = tmp.median(axis=0)\n",
    "        med.loc[HORIZON*60] = med.iloc[-1]\n",
    "        low = tmp.quantile(0.25)\n",
    "        low.loc[HORIZON*60] = low.iloc[-1]\n",
    "        up = tmp.quantile(0.75, axis=0)\n",
    "        up.loc[HORIZON*60] = up.iloc[-1]\n",
    "        plt.plot(med.index, med.to_numpy(), label=valid_pretty[HORIZON][mode], linewidth=3)\n",
    "        plt.fill_between(med.index, low, up, alpha=0.3)\n",
    "    plt.title('Name: %s (%d), #instances: %d, #attributes: %d' % (\n",
    "        tasks.loc[tid, 'name'], tid, tasks.loc[tid, 'NumberOfInstances'], tasks.loc[tid, 'NumberOfFeatures']))\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.ylim([plt.ylim()[0], plt.ylim()[0] + 0.3*(plt.ylim()[1]-plt.ylim()[0])])\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot average BER across all datasets\n",
    "for use_ensemble in ((True, ), (False, ), (True, False)):\n",
    "    for HORIZON in (10, 60):\n",
    "        model_list = []\n",
    "        for m in valid_pretty[HORIZON].keys():\n",
    "            if m[1] in use_ensemble:\n",
    "                model_list.append(m)\n",
    "\n",
    "        average_plot(model_list=model_list, res_dc=res_dc, valid_pretty=valid_pretty,\n",
    "                     horizon=HORIZON, task_ids_sorted_by_num_features=task_ids_sorted_by_num_features,\n",
    "                     min_diff_dc=min_diff_dc, figsize=(10, 5), legend=False)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/tmp/RQ1_%sMIN_%s_perf.pdf' % (\n",
    "            str(HORIZON), \"ens\" if use_ensemble[0] is True and len(use_ensemble) == 1 else \"other\")\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have either all or no models\n",
    "horizon_list = sorted(list(valid_pretty.keys()))\n",
    "model_list = {}\n",
    "for h in horizon_list:\n",
    "    model_list[h] = []\n",
    "    for m in valid_pretty[h].keys():\n",
    "        model_list[h].append(m)\n",
    "\n",
    "tab_data, stat_test_data = collect_data_for_final_table(model_list, res_dc, valid_pretty, horizon_list,\n",
    "                                                        task_ids_sorted_by_num_features, min_diff_dc)\n",
    "not_different = do_wilcoxon_test(stat_test_data, model_list, horizon_list, valid_pretty, exclude=[\"Oracle\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tab_data)\n",
    "#for horizon in horizon_list:\n",
    "#    df['Rank_%s' % horizon] = df[horizon].rank(method='average', ascending=True)\n",
    "print(df)\n",
    "print(df.to_latex())\n",
    "print(not_different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have either all or no models\n",
    "horizon_list = sorted(list(valid_pretty.keys()))\n",
    "model_list = {}\n",
    "for h in horizon_list:\n",
    "    model_list[h] = []\n",
    "    for m in valid_pretty[h].keys():\n",
    "        if valid_pretty[h][m] in (\"Auto-sklearn (2.0)\", \"Selector (no ens in sel) - other dir\"):\n",
    "            model_list[h].append(m)\n",
    "\n",
    "tab_data, stat_test_data = collect_data_for_final_table(model_list, res_dc, valid_pretty, horizon_list,\n",
    "                                                        task_ids_sorted_by_num_features, min_diff_dc)\n",
    "not_different = do_wilcoxon_test(stat_test_data, model_list, horizon_list, valid_pretty, exclude=[\"Oracle\",])\n",
    "df = pd.DataFrame(tab_data)\n",
    "#for horizon in horizon_list:\n",
    "#    df['Rank_%s' % horizon] = df[horizon].rank(method='average', ascending=True)\n",
    "print(df)\n",
    "print(df.to_latex())\n",
    "print(not_different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for use_ensemble in ((True, ), (True, False), (False, )):\n",
    "    for HORIZON in (10, 60):\n",
    "        model_list = []\n",
    "        for m in valid_pretty[HORIZON].keys():\n",
    "            if m[1] in use_ensemble:\n",
    "                model_list.append(m)\n",
    "        rank(model_list, res_dc, valid_pretty, HORIZON, task_ids_sorted_by_num_features, n_iter=20, \n",
    "             steplength=int(HORIZON * 60 / 100), legend=False)\n",
    "        # plt.legend(fontsize=style_dc[\"fontsize\"], loc=\"upper right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/tmp/RQ1_%sMIN_%s_rank.pdf' % (\n",
    "            str(HORIZON), \"ens\" if use_ensemble[0] is True and len(use_ensemble) == 1 else \"other\")\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for HORIZON in (10, 60):\n",
    "    model_list = []\n",
    "    for m in valid_pretty[HORIZON].keys():\n",
    "        if m[0] == 'auto':\n",
    "            model_list.append(m)\n",
    "    rank(model_list, res_dc, valid_pretty, HORIZON, task_ids_sorted_by_num_features, \n",
    "         n_iter=200, steplength=int(HORIZON * 60 / 100), paired=True, legend=False)\n",
    "    #plt.legend(fontsize=style_dc[\"fontsize\"], loc=\"center right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def table_per_dataset(model_list, res_dc, valid_pretty, horizon,\n",
    "                      task_ids_sorted_by_num_features, min_diff_dc):\n",
    "    # Generate data for final table and statistical testing\n",
    "    tab_data = {}\n",
    "\n",
    "    for mode in model_list:\n",
    "        assert mode in valid_pretty[horizon], (mode, valid_pretty[horizon].keys())\n",
    "        tab_data[valid_pretty[horizon][mode]] = {}\n",
    "        # Use label, not actual key\n",
    "        # Get means per tid\n",
    "        for tid in task_ids_sorted_by_num_features:\n",
    "            tmp = pd.DataFrame(res_dc[horizon][tid][mode]).sort_index(axis=1).ffill(axis=1).iloc[:, -1]\n",
    "            assert tmp.shape == (10, )\n",
    "            tmp = (tmp - min_diff_dc[tid][0]) / min_diff_dc[tid][1]\n",
    "            tab_data[valid_pretty[horizon][mode]][tid] = tmp.mean()\n",
    "    tab_data = pd.DataFrame(tab_data)\n",
    "    return tab_data\n",
    "\n",
    "h = 60\n",
    "horizon_list = sorted(list(valid_pretty.keys()))\n",
    "model_list = []\n",
    "for m in valid_pretty[h].keys():\n",
    "    if m[1]:\n",
    "        model_list.append(m)\n",
    "\n",
    "tab_data = table_per_dataset(model_list, res_dc, valid_pretty, h,\n",
    "                             task_ids_sorted_by_num_features, min_diff_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tab_data.copy()\n",
    "print(tmp.mean())\n",
    "tmp.drop(189873, inplace=True)\n",
    "print(tmp.mean())\n",
    "tmp.drop(189874, inplace=True)\n",
    "print(tmp.mean())\n",
    "tmp.drop(75193, inplace=True)\n",
    "print(tmp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tab_data[[\"Auto-sklearn (1.0)\", \"Auto-sklearn (1.0, random search)\"]]\n",
    "tmp['diff'] = tab_data[\"Auto-sklearn (1.0)\"] - tab_data[\"Auto-sklearn (1.0, random search)\"]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(tmp['diff'].to_numpy())\n",
    "plt.plot((0, 38), (0, 0))\n",
    "plt.xlabel('Dataset index (sorted from smallest to largest)')\n",
    "plt.ylabel('loss(ASKLv1(random,full) - loss(ASKLv2))')\n",
    "plt.title('ASKLv1 vs ASKLv2 for %d minutes' % h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
